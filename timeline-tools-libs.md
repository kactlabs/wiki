/ [Home](index.md)

## Template

**Note:** PraisonAI Timeline

### PraisonAI
- **Apr 2020** - PraisonAI development begins as an open-source framework for multi-agent LLM systems.
- **Sep 2020** - Integration of AutoGen and CrewAI to enable low-code, customizable solutions.
- **Jan 2021** - User Interfaces (UI) for multi-agent interaction, codebase engagement, and chat introduced.
- **Jun 2021** - Real-time voice interaction functionality added for enhanced user experience.
- **Nov 2021** - YAML-based configuration introduced for defining roles, tasks, and dependencies.
- **Mar 2022** - Custom tool integration and fine-tuning capabilities launched for personalized AI applications.
- **Aug 2022** - Source code and comprehensive documentation released on GitHub for community access.
- **2023â€“2024** - Continuous updates and improvements based on community feedback and advancements in AI technologies.


### ModernBERT
ModernBERT is a state-of-the-art encoder-only Transformer model designed to enhance and replace the original BERT architecture. Here's a timeline of its development:

- **December 18, 2024**: The research paper titled "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference" is submitted to arXiv, detailing the architecture and capabilities of ModernBERT.

- **December 19, 2024**: Hugging Face publishes a blog post introducing ModernBERT, highlighting its improvements over older encoder models, including support for sequences up to 8,192 tokens, better downstream performance, and faster processing.

- **December 19, 2024**: LightOn releases a blog post discussing ModernBERT's advancements in knowledge retrieval and classification, emphasizing its efficiency and performance in handling large context sizes and code data.

- **December 19, 2024**: The ModernBERT model is made available on Hugging Face's Model Hub, providing access to both base (149M parameters) and large (395M parameters) versions for public use.

- **December 24, 2024**: Simon Willison publishes a blog post summarizing ModernBERT's features and its significance as a replacement for BERT, noting its training on 2 trillion tokens and the incorporation of recent advancements in Transformer architectures.

ModernBERT represents a significant leap in encoder-only models, offering enhanced performance, extended context handling, and efficient processing, making it a valuable tool for various natural language processing tasks.


### GLINER
GLiNER is a compact and efficient Named Entity Recognition (NER) model designed to identify any entity type using a bidirectional transformer encoder. Here's a timeline of its development:

- **November 14, 2023**: The research paper titled "GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer" is submitted to arXiv, detailing the architecture and capabilities of GLiNER.

- **June 2024**: The paper is presented at the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2024) in Mexico City, Mexico.

- **November 30, 2024**: A blog post is published on Zilliz's website, discussing GLiNER's approach to NER and its impact on the NLP domain.

- **December 2024**: GLiNER is made available on Hugging Face's Model Hub, providing access to various versions for public use.

GLiNER represents a significant advancement in NER, offering flexibility and efficiency in identifying arbitrary entities across various domains and languages.

